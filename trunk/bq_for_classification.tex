\documentclass[twoside]{article}
\usepackage{aistats2e}

% For figures
%\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{preamble}
\usepackage{natbib}




\usepackage{usbib}
\bibliographystyle{myusmeg-a}
\renewcommand{\citenamefont}[1]{\textsc{\MakeLowercase{#1}}}
\renewcommand{\bibnamefont}[1]{\textsc{#1}}
\renewcommand*{\bibname}{biblography}

%\DeclareCaptionType{copyrightbox}

\begin{document}

\twocolumn[

\aistatstitle{Active learning of }

\aistatsauthor{ 
%Michael A. Osborne \And Roman   Garnett
}
\aistatsaddress{ 
% Department of Engineering Science \\
% University of Oxford \\
% Oxford OX1 3PJ, UK\\
% \url{mosb@robots.ox.ac.uk} 
\And 
}
]

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
Estimating multivariate Gaussian cumulative distribution functions is a problem with broad relevance. One example is Gaussian process classification, where the problem is usually tackled using Expectation Propagation. We propose an alternative method built around Bayesian quadrature; we use observations of convolutions of the Gaussian to perform inference for the desired Gaussian integral. We additionally describe a procedure to select the most informative observations by minimising the expected variance in the Gaussian integral. We demonstrate our method both for synthetic Gaussian integrals, and on a real Gaussian process classification problem. 
\end{abstract}


\section{Introduction}

\begin{itemize}
 \item error function is non-elementary, but efficient means for its computation exist. 
\item approximations for Gaussian process classification \citep{kuss2005assessing,nickisch2008approximations}
\item Note that we use a condensed notation; where limits for integrals are not supplied, they represent definite integrals over the entire domain. 
\begin{equation}
 \int \cdot \ud \v{x} \deq \int \ldots \int \ud x_D \ldots \ud x_1
\end{equation}
\item gaussian integrals \citep{cunningham2011approximate}
\item similar work in VB \citep{gershman2012nonparametric} (using a Gaussian mixture, similar to a GP with Gaussian covariance, uses observations of $\int q(\vx) \log g(\vx) \ud\vx$.
\end{itemize}




\section{Gaussian process classification}

\begin{figure}
\caption{Gaussian process classification}
\end{figure}


We motivate the relevance of Gaussian integrals to \gp classification.



We assume hyperparameters are known.
\begin{align}
\p{f\st}{\vy}
& =
\frac{\int\p{f\st}{\vf}\p{\vy}{\vf}p(\vf)\,\ud\vf}
{\int \p{\vy}{\vf}p(\vf)\ud\vf}
\\
\p{y\st}{\vy}
& =
\frac{\iint\p{y\st}{f\st}\p{f\st}{\vf}\p{\vy}{\vf}p(\vf)\ud f\st\ud\vf}
{\int\p{\vy}{\vf}p(\vf)\ud\vf}\,.
\end{align}
\begin{equation}
\p{\vy}{\vf} \deq \prod_{d=1}^{D}\p{y_d}{f_d}
\end{equation}



For classification, we adopt the step function likelihood
\begin{align}
 \p{y_d=1}{f_d} & \deq
\begin{cases}
1, & f_d>0\\
0, & f_d\leq 0\,.
\end{cases}
\intertext{Note that the step function likelihood with Gaussian noise in the discriminant, $f$, is equivalent to the probit likelihood with a noiseless discriminant. More generally, we consider the rectangular likelihood to allow censored regression}
 \p{y_d=1}{f_d} & \deq
\begin{cases}
1, & l_d\leq f_d<u_d\\
0, & \text{otherwise},
\end{cases}
\end{align}
we work with the latter henceforth.

\section{Bayesian Quadrature} \label{sec:bq}

% Note that maximum likelihood is also subject to issues. $\p{D}{x,I}$, how come (known) $I$ is on the right and (known) $D$ is on the left? 

\emph{Bayesian quadrature} \citep{BZHermiteQuadrature,BZMonteCarlo} is a means of performing Bayesian inference about the value of a potentially nonanalytic integral, \begin{equation}\label{eq:bq_integral}
\inty{g} \deq \int g(\vx) p(\vx) \ud \vx
\end{equation}


Quadrature involves evaluating $g(\vx)$ at a matrix of sample points $X$, giving $\v{g}\deq g(X)$. Often this evaluation is computationally expensive; the consequent sparsity of samples introduces uncertainty about the function $f$ between them, and hence uncertainty about the integral $\inty{g}$.

Most work on Bayesian quadrature chooses a \gp prior for $g$, with zero mean and the Gaussian covariance 
\begin{align} \label{eq:Gaussian_cov_fn}
k(\vx,\vx') & \deq \rho^2 \N{\vx}{\vx'}{\Omega}\,.
\end{align} 
This covariance, firstly, has hyperparameter $\rho$, 
termed the output scale. The input covariance $\Omega$ is also parameterised by further hyperparameters: in this, as in previous work, we'll take $\Omega$ as diagonal, with its diagonal elements defined by input, or length, scale hyperparameters.  These hyperparameters are typically fitted using type two maximum likelihood.

Variables possessing a multivariate Gaussian distribution are jointly Gaussian distributed with any affine transformations of those variables. Because integration is affine, we can hence use computed samples $\v{g}$ to perform analytic Gaussian process inference about the value of integrals over $g(x)$, such as $\inty{g}$. The mean estimate for $\inty{g}$ given $\v{g}$ is
%
\begin{align} 
&
\mean{\inty{g}}{\v{g}} 
\nonumber\\
& =\iint \inty{g}~\p{\inty{g}}{g}~\p{g}{\v{g}}~\ud \inty{g}~\ud g                                                                                                                                                               \nonumber\\
&
 =\iint \inty{g}\,\delta\bigl(\inty{g} - {\textstyle\int g(\vx)\,p(\vx)\,\ud x}\bigr)
\nonumber\\
& \hspace{4cm}
\N{g}{m_{g|X}}{C_{g|X}}\,\ud \inty{g} \,\ud g 
\nonumber\\
&
 = \int m_{g|X}(\vx)~p(\vx)~\ud \vx
\,, \label{eq:mean_inty}
\end{align}
and, similarly, the variance is
\begin{align} 
V(\inty{g}\mid\v{g})
=\iint p(\vx)~C_{g|X}(\vx, \vx')~p(\vx')~\ud \vx~\ud \vx'
\,. \label{eq:var_inty}
\end{align}
Here we've used the following dense notation for the standard \gp expressions for the posterior mean $m$ and covariance $C$ respectively:
\begin{align}
m_{g|X}(\vx)& \deq k(\vx, X) k(X, X)^{-1} \v{g}\\
C_{g|X}(\vx, \vx')& \deq k(\vx, \vx') - k(\vx, X) k(X, X)^{-1} k(X, \vx').
\end{align}
If the input density, $p(\vx)$, is Gaussian, \eqref{eq:mean_inty} is expressible in closed-form using standard Gaussian identities \citep{BZMonteCarlo}.
The use of an importance re-weighting trick ($q(\vx) = \nicefrac{q(\vx)}{p(\vx)} p(\vx)$ for any $q(\vx)$) allow any other integral to be approximated. 
%
The corresponding closed-form expression for the posterior variance of $\inty{g}$ lends itself as a natural convergence diagnostic. 

\section{Bayesian quadrature for Gaussian integrals}

\begin{figure}
\psfragfig[crop=pdfcrop]{figures/observations}
\caption{Gaussian integral}
\end{figure}


Consider the general Gaussian integral problem
\begin{align}\label{eq:GI}
\inty{g} & \deq \int_{\vl}^{\vu} \N{\vf}{\vmu}{\Sigma} \ud \vf\nonumber\\
& \deq \int_{l_1}^{u_1}\ldots\int_{l_D}^{u_D} \N{\vf}{\vmu}{\Sigma} \ud f_D \ldots \ud f_1\,.
\end{align}
In order to match \eqref{eq:GI} to \eqref{eq:bq_integral}, and hence to use \bq, we firstly define 
\begin{equation}
 g(\vf) \deq \N{\vf}{\vmu}{\Sigma}\,
\end{equation}
and adopt the improper input distribution, $p(\vf) = 1$. 

We now assign a \gp prior 
to the function $g: \reals^D \to \reals$, as in traditional \bq,
\begin{equation}
 p(g) = \mathcal{GP}\bigl(g; 0 , k(\cdot,\cdot)\bigr).
\end{equation}
 If we take $k$ as a stationary covariance, as is typical, the predictive variance for $\inty{g}$ is infinite: the $\iint k(\vx,\vx')\,\ud\vx\,\ud \vx'$ term in \eqref{eq:var_inty} is unbounded, regardless of $X$. Intuitively, given finite observations of $g$, there will always be an infinite amount of potentially unexplored mass of the integrand, $g(\vx)p(\vx)$. This infinite variance is not reflective of our true state of knowledge, however. We know that the light tails of the Gaussian cause $g$ to be arbitrarily small sufficiently far from $\v{\mu}$, bounding how much mass can have been missed. We can express this knowledge by using a more appropriate covariance. Note that if we have a Gaussian prior $\N{a}{\alpha}{\beta^2}$ for an arbitrary variable $a$, the prior for its product $c = a b$ with known $b$ is $\N{c}{b \alpha}{b \beta^2 b}$. In our case, we model $g$ as being the product of an unknown function, possessing the Gaussian covariance \eqref{eq:Gaussian_cov_fn}, and an isotropic Gaussian $\N{f}{\vmu}{\Lambda}$. This Gaussian represents a bounding envelope for $g$; we expect it to drop to zero more slowly than $g$. This requirement implies that $\Lambda - \Sigma$ must be positive semi-definite; if we further require the tightest such bound, we must simply solve the semi-definite program
\begin{align}
 \min_{\v{\lambda}\in\reals^D} & \sum_i \lambda_i\\
\text{subject to } & \diag{\v{\lambda}} - \Sigma \succeq 0~.
\end{align}
Here we have defined $\Lambda = \diag \v{\lambda}$, which both simplifies the semi-definite program and allows tractable inference, as we'll see later. With this $\Lambda$, we arrive at the covariance for $g$
\begin{align}
 & \text{cov}\bigl(g(\vf), g(\vf')\bigr) = k(\vf, \vf')\nonumber\\ & \deq \rho^2~\N{\vf}{\vmu}{\Lambda}~\N{\vf}{\vf'}{\Omega}~\N{\vf'}{\vmu}{\Lambda}\,.
\end{align}

Another difference to traditional \sc{bq} is the nature of the observations available to us. As for standard \sc{bq}, we can evaluate $g(\vf)$ for any $\vf$, a \emph{point observation}. However, due to the curse of dimensionality, we are likely to require a very great number of such point evaluations in order to perform accurate inference for $\inty{g}$ in high dimension $D$. Similar approaches built around Monte Carlo sampling \citep{genz1992numerical} exist, but are similarly recognised as requiring a prohibitively large number of samples for high dimension. 

An alternative is the class of \emph{slice observations}
\begin{equation}
 \int \ldots \int \int_{l_d}^{u_d} \int \ldots \int g(\vf)~\ud f_D \ldots \ud f_d\ldots \ud f_1,
\end{equation}
which are expressible using standard error functions. Just as for sample evaluations, however, such observations are not very informative in high dimension. The problem for both types of observation is that, for increasingly high $D$, they are informative of only an increasingly tiny quantity of the domain over which $g$ must be integrated. What is needed is an observation of another integral over the whole of $\vf$.

Such an integral that is expressible in closed form is the Gaussian \emph{convolution observation}
 \begin{equation}\label{eq:conv}
 c_i = \int \N{\vf}{\vm_i}{V_i}~g(\vf)~\ud \vf = \N{\vm_i}{\vmu}{V_i+\Sigma}.
\end{equation}
 If the convolving Gaussian is `close' to the product of top hat functions required to reproduce \eqref{eq:GI}, then such convolutions may permit useful inference for $\inty{g}$. The covariance between $\inty{g}$ and the convolution $c_i$ is
\begin{align}\label{eq:Ksti}
\text{cov}\bigl(\inty{g}, c_i\bigr) =
\int_{\vl}^{\vu} \int  k(\vf, \vf')~\N{\vf'}{\vm_i}{V_i}~\ud \vf' \ud \vf 
\end{align}
While we can compute $c_i$ for arbitrary $V_i$, the computation of \eqref{eq:Ksti} places constraints on our model. In particular, tractability requires that $\Omega$, $\Lambda$ and $V_i$ are all diagonal, so that \eqref{eq:Ksti} becomes the product of $D$ independent two-dimensional integrals.

We restrict ourselves to consideration of such convolution observations henceforth.


\section{Active Bayesian Quadrature for Gaussian Integrals}




\bibliography{bub}
\end{document}

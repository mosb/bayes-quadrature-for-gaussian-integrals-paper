\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}

% For figures
%\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{preamble}

\catcode`\@=11
\DeclareCaptionType{copyrightbox}


\jmlrheading{1}{-}{-}{-}{-}{Michael Osborne}

% Short headings should be running head and authors last names

\ShortHeadings{Learning with Mixtures of Trees}{Meil\u{a} and Jordan}
\firstpageno{1}

\begin{document}

\title{Bayesian Quadrature for Classification}

\author{\name Michael Osborne \email mosb@robots.ox.ac.uk \\
       \addr Department of Engineering Science\\
       University of Oxford\\
       Ocford OX1 3PJ, UK}

\editor{-}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
\end{abstract}

\begin{keywords}
\end{keywords}

\section{Introduction}

\begin{itemize}
 \item error function is non-elementary, but efficient means for its computation exist. 
 \item PCA
 \item acknoeldge Mackay's humble gaussian objection to eigenvectors, but the latent function used for GP classificaiton is dimensionless (or else the logistic, for example, wouldn't work).
\item ignore correlations between numerator and denominator integrals
\end{itemize}

We assume hyperparameters are known.
\begin{align*}
\p{\vf\st}{\vy_d}
& =
\frac{\int\p{\vf\st}{\vf_d}\p{\vy_d}{\vf_d}p(\vf_d)\,\ud\vf_d}
{\int \p{\vy_d}{\vf_d}p(\vf_d)\ud\vf_d}
\\
\p{\vy\st}{\vy_d}
& =
\frac{\iint\p{\vy\st}{\vf\st}\p{\vf\st}{\vf_d}\p{\vy_d}{\vf_d}p(\vf_d)\ud\vf\st\ud\vf_d}
{\int\p{\vy_d}{\vf_d}p(\vf_d)\ud\vf_d}\,.
\end{align*}
\begin{align*}
\p{\vy_d}{\vf_d} & \deq \prod_{i=1}^{\card{d}}\p{y_i}{f_i} \\
\p{\vy\st}{\vf\st} & \deq \prod_{i=1}^{\card{\star}}\p{y_i}{f_i}
\end{align*}



For classification, we adopt the step function likelihood
\begin{align*}
 \p{y_i=1}{f_i} & \deq
\begin{cases}
1, & f_i>0\\
0, & f_i\leq 0\,.
\end{cases}
\intertext{More generally, we consider the rectangular likelihood to allow censored regression}
 \p{y_i=1}{f_i} & \deq
\begin{cases}
1, & l_i\leq f_i<u_i\\
0, & \text{otherwise},
\end{cases}
\end{align*}
we work with the latter henceforth.

Use $S$ to represent a matrix whose columns define the vertices of a polytope enclosing the region of integration.
\begin{align*}
 \int_S \N{\vf}{\vm}{C}\ud \vf 
& = \int_{V^{-1}S} \N{\vf}{V^{-1}\vm}{E}\ud \vf 
\end{align*}




%\bibliography{bub}
\end{document}
